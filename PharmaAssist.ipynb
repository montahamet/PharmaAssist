{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_chroma langchain_groq langchain_core langchain_community langchain_text_splitters pypdf gradio sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters.sentence_transformers import SentenceTransformersTokenTextSplitter\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for persistent DB\n",
    "os.makedirs(\"pharma_db\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model and vector DB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "db = Chroma(collection_name=\"pharma_database\", embedding_function=embedding_model, persist_directory=\"pharma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a highly knowledgeable assistant specializing in pharmaceutical sciences.\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Answer the question based on the above context:\n",
    "{question}\n",
    "\n",
    "Use the provided context to answer the user's question accurately and concisely.\n",
    "Don't justify your answers.\n",
    "Don't give information not mentioned in the CONTEXT INFORMATION.\n",
    "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process uploaded PDF files\n",
    "def process_documents(file_paths):\n",
    "    for file_path in file_paths:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        data = loader.load()\n",
    "\n",
    "        doc_metadata = [doc.metadata for doc in data]\n",
    "        doc_content = [doc.page_content for doc in data]\n",
    "\n",
    "        text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        chunks = text_splitter.create_documents(doc_content, doc_metadata)\n",
    "        db.add_documents(chunks)\n",
    "\n",
    "    return \"‚úÖ Documents processed and added to database.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Query using RAG \n",
    "def run_query(query, groq_api_key):\n",
    "    # Set up the retriever. \n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}) \n",
    "    \n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        api_key=groq_api_key,\n",
    "        temperature=1\n",
    "    )\n",
    "\n",
    "    # Define the RAG chain\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt_template\n",
    "        | llm\n",
    "        | output_parser\n",
    "    )\n",
    "\n",
    "    result = rag_chain.invoke(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Artificial intelligence (AI) applications in drug discovery include: \n",
      "1. Identifying promising drug candidates by analyzing large chemical databases using machine learning.\n",
      "2. Predicting molecular interactions and drug-target affinities using deep learning models.\n",
      "3. Patient stratification in clinical trials.\n",
      "4. Drug repurposing efforts. \n"
     ]
    }
   ],
   "source": [
    "# Example Usage \n",
    "if __name__ == \"__main__\":\n",
    "    # Optional: Process some PDF files first\n",
    "    pdf_paths = [\"/content/sample_data/AI_in_Drug_Discovery.pdf\", \"/content/sample_data/Vaccine_Development_Workflow.pdf\"]\n",
    "    # This line assumes you have 'sample_data/AI_in_Drug_Discovery.pdf' and 'sample_data/Vaccine_Development_Workflow.pdf' files available\n",
    "    # Note: If these files don't exist, this cell will fail.\n",
    "    process_documents(pdf_paths) \n",
    "\n",
    "    # Run a query\n",
    "    groq_api_key = \"YOUR_GROQ_API_KEY\" # **REPLACE with your actual key**\n",
    "    user_query = \"What are the AI applications in drug discovery?\"\n",
    "    answer = run_query(user_query, groq_api_key)\n",
    "    print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Note: The imports and core functions (format_docs, process_documents, run_query) are assumed to be defined in previous cells.\n",
    "\n",
    "# Main Gradio interface function\n",
    "def pharma_query_interface(query, groq_api_key, files):\n",
    "    if files:\n",
    "        # Process new documents if uploaded\n",
    "        process_documents(files)\n",
    "        \n",
    "    if not query or not groq_api_key:\n",
    "        return \"‚ö†Ô∏è Please enter a query and your GROQ API key.\"\n",
    "        \n",
    "    return run_query(query, groq_api_key)\n",
    "\n",
    "# Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=pharma_query_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Pharmaceutical Question\", placeholder=\"e.g., What are the AI applications in drug discovery?\"),\n",
    "        gr.Textbox(label=\"Groq API Key\", type=\"password\"),\n",
    "        gr.File(label=\"Upload PDF documents (optional)\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"RAG Answer\", lines=10),\n",
    "    title=\"üíä PharmaAssist - RAG for Pharmaceutical Research\",\n",
    "    description=\"Upload pharmaceutical research PDFs and ask questions using Groq's LLaMA3 and HuggingFace Embeddings.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    # Note: Gradio interfaces often require a restart of the kernel in notebook environments to stop cleanly.\n",
    "    iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
